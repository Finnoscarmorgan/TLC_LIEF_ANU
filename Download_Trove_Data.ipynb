{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fiannualamorgan/anaconda3/envs/spatial_analysis/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import fuzzysearch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from IPython.display import HTML, display\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests_cache.CachedSession()\n",
    "retries = Retry(total=10, backoff_factor=0.2, status_forcelist=[500, 502, 503, 504, 403], allowed_methods=[\"GET\"])\n",
    "s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "s.mount(\"https://\", HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results retrieved: 247\n"
     ]
    }
   ],
   "source": [
    "api_key = \"API KEY HERE\"  \n",
    "\n",
    "api_url = \"https://api.trove.nla.gov.au/v3/result\"\n",
    "\n",
    "# Initialize an empty list to store all the results\n",
    "all_results = []\n",
    "\n",
    "# Initial params for the first request\n",
    "params = {\n",
    "    \"q\": \"Eikon Basilike\",\n",
    "    \"holding\": \"true\",\n",
    "    \"category\": \"book\",\n",
    "    \"code\": \"book\",\n",
    "    \"name\": \"Books & Libraries\",\n",
    "    \"encoding\": \"json\",\n",
    "    \"reclevel\": \"full\",\n",
    "    \"key\": api_key,\n",
    "    \"n\": 100,  # You can set this to the maximum allowed, which seems to be 100\n",
    "    \"include\": \"workversions\",\n",
    "    \"include\": \"holdings\",\n",
    "}\n",
    "\n",
    "while True:\n",
    "    response = requests.get(api_url, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error {response.status_code}: {response.content}\")\n",
    "        break\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract magazine_data using the corrected path\n",
    "    trove_data = data.get(\"category\", [{}])[0].get(\"records\", {})\n",
    "    \n",
    "    all_results.extend(trove_data.get(\"work\", []))\n",
    "    \n",
    "    # Check if there's a nextStart value to continue fetching more results\n",
    "    if \"nextStart\" in trove_data:\n",
    "        params[\"s\"] = trove_data[\"nextStart\"]\n",
    "    else:\n",
    "        break  # No more results to fetch, exit the loop\n",
    "\n",
    "print(\"Total results retrieved:\", len(all_results))\n",
    "\n",
    "# Save all the results to a JSON file\n",
    "with open(\"trove_data_dump_download.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(all_results, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to output_5_Nov_2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('trove_data_dump_download.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define the CSV file where the output will be saved\n",
    "csv_file = 'WRITE FILE HERE'\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Define the CSV column names\n",
    "    fieldnames = [\n",
    "        'id', 'troveUrl', 'title', 'contributor', 'format', 'extent',\n",
    "        'issued', 'holdingsCount', 'versionCount', 'nuc', 'holding_url',\n",
    "        'call_number_value', 'local_identifier'\n",
    "    ]\n",
    "\n",
    "    # Create a CSV DictWriter object\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Iterate over each entry in the JSON data\n",
    "    for entry in data:\n",
    "        # Extract the relevant fields from the JSON entry\n",
    "        id = entry.get('id')\n",
    "        url = entry.get('troveUrl')\n",
    "        title = entry.get('title')\n",
    "        contributor = entry.get('contributor')  # Extracting contributor\n",
    "        format_type = entry.get('format')  # Extracting format\n",
    "        extent = entry.get('extent')  # Extracting extent\n",
    "        issued = entry.get('issued')  # Extracting issued date\n",
    "        holdings_count = entry.get('holdingsCount')\n",
    "        version_count = entry.get('versionCount')\n",
    "\n",
    "        # Iterate over each 'holding' in the entry\n",
    "        for holding in entry.get('holding', []):\n",
    "            nuc = holding.get('nuc')\n",
    "            holding_url = holding.get('url', {}).get('value')\n",
    "\n",
    "            # Check if there are call numbers and iterate through them\n",
    "            for call_number_info in holding.get('callNumber', []):\n",
    "                call_number_value = call_number_info.get('value')\n",
    "                local_identifier = call_number_info.get('localIdentifier')\n",
    "\n",
    "                # Write the row to the CSV file\n",
    "                writer.writerow({\n",
    "                    'id': id,\n",
    "                    'troveUrl': url,\n",
    "                    'title': title,\n",
    "                    'contributor': contributor,\n",
    "                    'format': format_type,\n",
    "                    'extent': extent,\n",
    "                    'issued': issued,  # Adding issued to the row\n",
    "                    'holdingsCount': holdings_count,\n",
    "                    'versionCount': version_count,\n",
    "                    'nuc': nuc,\n",
    "                    'holding_url': holding_url,\n",
    "                    'call_number_value': call_number_value,\n",
    "                    'local_identifier': local_identifier\n",
    "                })\n",
    "\n",
    "print(f'Data written to {csv_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 154 unique values in the id column\n",
      "There are 31 unique values in the nuc column\n"
     ]
    }
   ],
   "source": [
    "#load in the csv file 'Edited_Eikon_Spreadsheet.csv' from current directory\n",
    "df = pd.read_csv('Edited_Eikon_Spreadsheet.csv')\n",
    "#count unique values in the 'id' column, print this as \"there are x unique values in the id column\"\n",
    "print(f'There are {df[\"id\"].nunique()} unique values in the id column for the edited spreadsheet')\n",
    "#count number of unique values in the 'nuc' column\n",
    "print(f'There are {df[\"nuc\"].nunique()} unique values in the nuc column for the edited spreadsheet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 unique values in the nuc column\n",
      "There are 206 unique values in the id column\n"
     ]
    }
   ],
   "source": [
    "#load in the spreadsheet output_5_Nov_2.csv from current directory\n",
    "df2 = pd.read_csv('output_5_Nov_2.csv')\n",
    "#count number of unique values in the 'nuc' column\n",
    "print(f'There are {df2[\"nuc\"].nunique()} unique values in the nuc column for the original spreadsheet')\n",
    "#count unique values in the 'id' column, print this as \"there are x unique values in the id column\"\n",
    "print(f'There are {df2[\"id\"].nunique()} unique values in the id column for the original spreadsheet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatial_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
